{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recipe Scraper for allrecipes.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input of the model\n",
    "#### 1. homepage\n",
    "\t- This is the recipe website. \n",
    "    - eg. 'https://www.allrecipes.com/recipes/78/breakfast-and-brunch/'\n",
    "#### 2. numofpage\n",
    "    - This is the maximum number of pages. \n",
    "    - eg. 'https://www.allrecipes.com/recipes/78/breakfast-and-brunch/?page=194'\n",
    "#### 3. group_name\n",
    "    - This is the category of scraped recipes. It should be determined by the programmer manually. \n",
    "    - eg. breakfast\n",
    "#### 4. home_path\n",
    "    - This is the path you want to save recipe data. \n",
    "    - eg. ../data/\n",
    "\n",
    "By changing above 4 input, the recipe data can be scraped and saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib import error\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from urllib.robotparser import RobotFileParser\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = pd.read_excel('model_input.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(model_input)):\n",
    "    homepage = model_input['homepage'][i]\n",
    "    numofpage = int(model_input['numofpage'][i])\n",
    "    group_name = model_input['group_name'][i]\n",
    "    home_path = model_input['home_path'][i]\n",
    "    def geturls(page):\n",
    "        dietpage = urlopen(recipepage[page])\n",
    "        dietsoup = BeautifulSoup(dietpage,'html.parser') \n",
    "        urls = []\n",
    "        recipecard = dietsoup.find_all('div', attrs = {'class':'card__detailsContainer'})\n",
    "        for i in range(len(recipecard)):\n",
    "            recipe = recipecard[i]\n",
    "            recipelink = recipe.find_all('a', attrs = {'class':'card__titleLink manual-link-behavior'})\n",
    "            for link in recipelink:\n",
    "                urls.append(link.get('href'))\n",
    "        return urls\n",
    "\n",
    "    recipepage = []\n",
    "    recipepage.append(homepage) \n",
    "\n",
    "    for i in range(1,numofpage): \n",
    "        recipepage.append(homepage+'?page='+str(i+1)) \n",
    "\n",
    "    print(recipepage)\n",
    "\n",
    "    urls = []\n",
    "    for i in range(len(recipepage)):\n",
    "        try:\n",
    "            for m in range(10):\n",
    "                try:\n",
    "                    add = geturls(recipepage, i)\n",
    "                    urls = urls + add\n",
    "                    print('get page ' + str(i) + ' all the recipe urls')\n",
    "                    print(add)\n",
    "                    print('-------------------------')\n",
    "                    print(urls)\n",
    "                    time.sleep(0.1)\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    if m >= 9:\n",
    "                        print(e)\n",
    "                    else:\n",
    "                        time.sleep(0.1)\n",
    "        except Exception as e:\n",
    "            break\n",
    "    \n",
    "    if urls == []:\n",
    "        print('The webpage for this group of recipes is problematic.')\n",
    "        print('We skip this ' + group_name)\n",
    "        continue\n",
    "\n",
    "    class recipeinfo:\n",
    "        '''Yes'''\n",
    "        def __init__(self, url):\n",
    "            for i in range(10):\n",
    "                try:\n",
    "                    self.recipepage = urlopen(url)\n",
    "                    time.sleep(0.1)\n",
    "                    break            \n",
    "                except Exception as e:\n",
    "                    if i >= 9:\n",
    "                        print(e)\n",
    "                    else:\n",
    "                        time.sleep(0.1)\n",
    "            self.recipesoup = BeautifulSoup(self.recipepage,'html.parser')\n",
    "\n",
    "        '''Yes'''\n",
    "        def getname(self):\n",
    "            self.name = self.recipesoup.find(attrs = {'class':'headline heading-content'})\n",
    "            try:\n",
    "                self.name = self.name.string\n",
    "            except AttributeError:\n",
    "                self.name = 'unknown'\n",
    "            self.name = self.name.replace('/',' or ')\n",
    "            self.name = self.name.replace('\"', '')\n",
    "            return self.name\n",
    "\n",
    "\n",
    "        '''Yes'''\n",
    "        def getsummary(self):\n",
    "            try:\n",
    "                self.summary = self.recipesoup.find(attrs = {'class':'recipe-summary margin-8-bottom'})\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                self.summary = 'not known'\n",
    "            try:\n",
    "                self.paragraph = self.summary.find(attrs = {'class':'margin-0-auto'})\n",
    "                self.paragraph = self.paragraph.string\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                self.paragraph = 'not known'\n",
    "            return self.paragraph\n",
    "\n",
    "        '''Yes'''\n",
    "        def getauthor(self):\n",
    "            self.author = self.recipesoup.find(attrs = {'class':'author-name link'})\n",
    "            try: \n",
    "                self.author = self.author.string\n",
    "            except AttributeError:\n",
    "                self.author = 'unknown'\n",
    "            try:\n",
    "                self.authorlink = self.author.get('href')\n",
    "            except AttributeError:\n",
    "                self.authorlink = 'unknown'\n",
    "            return self.author\n",
    "            return self.authorlink\n",
    "\n",
    "        '''Yes'''\n",
    "        def getrecipeinfo(self):\n",
    "            try:\n",
    "                self.recipeinfo = self.recipesoup.find_all(attrs = {'class':'recipe-meta-item-body'})\n",
    "                self.infolist = []\n",
    "                for info in self.recipeinfo:\n",
    "                    info = info.string\n",
    "                    info = info.replace('\\n','')\n",
    "                    info = re.sub(' +', ' ', info)\n",
    "                    info = info.strip()\n",
    "                    self.infolist.append(info)\n",
    "                self.infolist[-2] = int(self.infolist[-2])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                self.infolist = [1,'none']\n",
    "            return self.infolist\n",
    "\n",
    "        '''Yes'''\n",
    "        def getingredients(self):\n",
    "            try:\n",
    "                self.ingredients = self.recipesoup.find_all(attrs = {'class':'ingredients-item-name'})\n",
    "                self.ingredientslist = []\n",
    "                for ingredient in self.ingredients:\n",
    "                    self.ingredientslist.append(ingredient.string)\n",
    "\n",
    "                for i in range(len(self.ingredientslist)):\n",
    "                    self.ingredientslist[i] = self.ingredientslist[i].replace('\\n','')\n",
    "                    self.ingredientslist[i] = re.sub(' +', ' ', self.ingredientslist[i])\n",
    "                    self.ingredientslist[i] = self.ingredientslist[i].strip()\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                self.ingredientslist = ['not known']\n",
    "            return self.ingredientslist\n",
    "\n",
    "        '''Yes'''\n",
    "        def getdirections(self):\n",
    "            try:\n",
    "                self.directions = self.recipesoup.find_all(attrs = {'class':'subcontainer instructions-section-item'})\n",
    "                self.directionslist = []\n",
    "                for direction in self.directions:\n",
    "                    direction = direction.find('div')\n",
    "                    direction = direction.find('p')\n",
    "                    direction = direction.string\n",
    "                    self.directionslist.append(direction)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                self.directionslist = ['not known']\n",
    "            return self.directionslist\n",
    "\n",
    "        '''Yes'''\n",
    "        def getnutrition(self):\n",
    "            try:\n",
    "                self.nutrition = self.recipesoup.find(attrs = {'class':'partial recipe-nutrition-section'})\n",
    "                self.nutrition = self.nutrition.find(attrs = {'class':'section-body'})\n",
    "                self.nutrition = self.nutrition.text\n",
    "                self.nutrition = self.nutrition.replace('Full Nutrition\\n','')\n",
    "                self.nutrition = re.sub(' +', ' ', self.nutrition)\n",
    "                self.nutrition = self.nutrition.strip()\n",
    "                self.nutritionlist = self.nutrition.split(';')\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                self.nutritionlist = [0,0,0,0,0,0]\n",
    "            self.nutritionfact = []\n",
    "            for i in range(len(self.nutritionlist)):\n",
    "                if i == 0:\n",
    "                    self.nutritionlist[i] = self.nutritionlist[i].replace(' calories','')\n",
    "                    try:\n",
    "                        self.nutritionlist[i] = float(self.nutritionlist[i])\n",
    "                    except Exception:\n",
    "                        self.nutritionlist[i] = 0\n",
    "                    self.nutritionfact.append(self.nutritionlist[i])\n",
    "                if i == 1:\n",
    "                    self.nutritionlist[i] = self.nutritionlist[i].replace(' protein ','')\n",
    "                    self.nutritionlist[i] = self.nutritionlist[i].replace(' DV','')\n",
    "                    temp = self.nutritionlist[i].split('g')\n",
    "                    try:  \n",
    "                        temp[1] = temp[1].strip()\n",
    "                        temp[0] = float(temp[0])\n",
    "                    except Exception:\n",
    "                        temp[0] = 0\n",
    "                    self.nutritionfact.append(temp[0])\n",
    "                if i == 2:\n",
    "                    self.nutritionlist[i] = self.nutritionlist[i].replace(' carbohydrates ','')\n",
    "                    self.nutritionlist[i] = self.nutritionlist[i].replace(' DV','')\n",
    "                    temp = self.nutritionlist[i].split('g')\n",
    "                    try:  \n",
    "                        temp[1] = temp[1].strip()\n",
    "                        temp[0] = float(temp[0])\n",
    "                    except Exception:\n",
    "                        temp[0] = 0\n",
    "                    self.nutritionfact.append(temp[0])\n",
    "                if i == 3:\n",
    "                    self.nutritionlist[i] = self.nutritionlist[i].replace(' fat ','')\n",
    "                    self.nutritionlist[i] = self.nutritionlist[i].replace(' DV','')\n",
    "                    temp = self.nutritionlist[i].split('g')\n",
    "                    try:  \n",
    "                        temp[1] = temp[1].strip()\n",
    "                        temp[0] = float(temp[0])\n",
    "                    except Exception:\n",
    "                        temp[0] = 0\n",
    "                    self.nutritionfact.append(temp[0])\n",
    "                if i == 4:\n",
    "                    self.nutritionlist[i] = self.nutritionlist[i].replace(' cholesterol ','')\n",
    "                    self.nutritionlist[i] = self.nutritionlist[i].replace(' DV','')\n",
    "                    temp = self.nutritionlist[i].split('mg')\n",
    "                    try:  \n",
    "                        temp[1] = temp[1].strip()\n",
    "                        temp[0] = float(temp[0])\n",
    "                    except Exception:\n",
    "                        temp[0] = 0\n",
    "                    self.nutritionfact.append(temp[0])\n",
    "                if i == 5:\n",
    "                    self.nutritionlist[i] = self.nutritionlist[i].replace(' sodium ','')\n",
    "                    self.nutritionlist[i] = self.nutritionlist[i].replace(' DV','')\n",
    "                    temp = self.nutritionlist[i].split('mg')\n",
    "                    try:  \n",
    "                        temp[1] = temp[1].strip()\n",
    "                        temp[0] = float(temp[0])\n",
    "                    except Exception:\n",
    "                        temp[0] = 0\n",
    "                    self.nutritionfact.append(temp[0])\n",
    "            return self.nutritionfact\n",
    "\n",
    "    # Each website is a separate project (folder)\n",
    "    def create_project_dir(directory):\n",
    "        if not os.path.exists(directory):\n",
    "            print('Creating directory ' + directory)\n",
    "            os.makedirs(directory)\n",
    "\n",
    "    path = home_path + group_name\n",
    "    create_project_dir(path) # '../data/breakfast'\n",
    "    create_project_dir(path + '/each_' + group_name)  # '../data/breakfast/each_breakfast'\n",
    "    output = []\n",
    "    iteration = 0\n",
    "\n",
    "    '''modify'''\n",
    "    def recipedata(start,urls,path,group_name):\n",
    "        '''########################## Make changes ############################'''\n",
    "        global iteration\n",
    "        global output\n",
    "        table = pd.DataFrame(columns = [\n",
    "                                'model','recipe_id','recipe_name','servings','group_name','calories',\n",
    "                                'protein','carbohydrates','fat','ingredients','recipe_source'\n",
    "                            ])\n",
    "\n",
    "        each = pd.DataFrame(columns = [\n",
    "                                'model','recipe_id','recipe_name','servings','group_name','calories',\n",
    "                                'protein','carbohydrates','fat','ingredients','recipe_source'\n",
    "                            ])    \n",
    "\n",
    "        for recipeid in range(start,len(urls)):\n",
    "            iteration = recipeid\n",
    "            print('Scraping the '+str(recipeid)+'th' + ' recipe')\n",
    "\n",
    "            information = recipeinfo(urls[recipeid])\n",
    "            information.getname()\n",
    "            information.getsummary()\n",
    "            information.getauthor()\n",
    "            information.getrecipeinfo()\n",
    "            information.getingredients()\n",
    "            information.getdirections()\n",
    "            information.getnutrition()\n",
    "\n",
    "            data = {'model': 'recipes.recipe',\n",
    "                    'recipe_id':recipeid+1,\n",
    "                    'recipe_name': information.name,\n",
    "                    'servings': information.infolist[-2],\n",
    "                    'group_name': group_name,\n",
    "                    'calories': information.nutritionfact[0],\n",
    "                    'protein': information.nutritionfact[1],\n",
    "                    'carbohydrates': information.nutritionfact[2],\n",
    "                    'fat': information.nutritionfact[3], \n",
    "                    'ingredients':information.ingredientslist,\n",
    "                    'recipe_source':urls[recipeid]\n",
    "                   }\n",
    "            output.append(data)\n",
    "            print('the len of urls is '+ str(len(urls))+'. the len of output is '+ str(len(output))+'.')\n",
    "\n",
    "            print('Save the accumulated recipes until the '+str(recipeid)+'th' + ' recipe to JSON format')\n",
    "            with open(path+'/'+group_name+'.json', mode='w', encoding='utf-8') as f: \n",
    "                json.dump(output, f)\n",
    "\n",
    "            print('Save the '+str(recipeid)+'th' + ' recipe to CSV format')\n",
    "\n",
    "            each = each.append([{'recipe':group_name + '-' + str(recipeid+1),'model':'recipes.recipe','recipe_id':recipeid+1,\n",
    "                                   'recipe_name':information.name,'servings':information.infolist[-2],\n",
    "                                   'group_name':group_name,\n",
    "                                   'calories':information.nutritionfact[0],\n",
    "                                   'protein':information.nutritionfact[1],\n",
    "                                   'carbohydrates':information.nutritionfact[2],\n",
    "                                   'fat':information.nutritionfact[3],\n",
    "                                   'ingredients':information.ingredientslist,\n",
    "                                   'recipe_source':urls[recipeid]}], ignore_index=True)\n",
    "\n",
    "            each.to_csv(path + '/each_'+group_name+'/'+information.name+'.csv')\n",
    "            each = pd.DataFrame(columns = [\n",
    "                                    'model','recipe_id','recipe_name','servings','group_name','calories',\n",
    "                                    'protein','carbohydrates','fat','ingredients','recipe_source'\n",
    "                                ])\n",
    "            print('Save the accumulated recipes until the '+str(recipeid)+'th' + ' recipe to CSV format')\n",
    "            table = table.append([{'recipe':group_name + '-' + str(recipeid+1),'model':'recipes.recipe','recipe_id':recipeid+1,\n",
    "                                   'recipe_name':information.name,'servings':information.infolist[-2],\n",
    "                                   'group_name':group_name,\n",
    "                                   'calories':information.nutritionfact[0],\n",
    "                                   'protein':information.nutritionfact[1],\n",
    "                                   'carbohydrates':information.nutritionfact[2],\n",
    "                                   'fat':information.nutritionfact[3],\n",
    "                                   'ingredients':information.ingredientslist,\n",
    "                                   'recipe_source':urls[recipeid]}], ignore_index=True)\n",
    "            table.to_csv(path+'/'+group_name+'.csv')\n",
    "            if len(output) >= len(urls):\n",
    "                break\n",
    "\n",
    "        print('Save all recipes as JSON file')\n",
    "        with open(path+'/'+group_name+'.json', mode='w', encoding='utf-8') as f:\n",
    "            json.dump(output, f)\n",
    "            # 将字典列表存入json文件中\n",
    "\n",
    "    '''modify'''\n",
    "    while True:\n",
    "        print('the len of urls is '+ str(len(urls))+'. the len of output is '+ str(len(output))+'.')\n",
    "        if len(output) >= len(urls):\n",
    "            break\n",
    "        try:\n",
    "            print('The input iteration is '+str(iteration))\n",
    "            print('The len of urls is '+ str(len(urls)))\n",
    "            recipedata(iteration,urls,path,group_name)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('The current iter is '+str(iteration)+'. Now input to RecipeJson again.')\n",
    "            urls.pop(iteration)\n",
    "            print('The problematic elements in urls has been removed. The len of urls is now '+str(len(urls))+'.')\n",
    "\n",
    "            with open(path+'/'+group_name+'.json', mode='r', encoding='utf-8') as f:\n",
    "                output = json.load(f)\n",
    "\n",
    "    '''length check'''\n",
    "    with open(path+'/'+group_name+'.json', mode = 'r', encoding = 'utf-8') as f:\n",
    "        output = json.load(f)\n",
    "    print('now the len of output is '+ str(len(output)))    \n",
    "    recipe_name = []\n",
    "    for i in range(len(output)):\n",
    "        recipe_name.append(output[i]['recipe_name'])\n",
    "    setrecipe_name = set(recipe_name)\n",
    "    print('the number of non-repeatable recipe name is' + str(len(setrecipe_name)))\n",
    "    if len(setrecipe_name) == len(recipe_name):\n",
    "        print('no repeatative elements')\n",
    "    else:\n",
    "        print('repeatation exists')\n",
    "\n",
    "    recipe_name = []\n",
    "    new = []\n",
    "    for i in range(len(output)):\n",
    "        if output[i]['recipe_name'] in recipe_name:\n",
    "            pass\n",
    "        else:\n",
    "            recipe_name.append(output[i]['recipe_name'])\n",
    "            new.append(output[i])\n",
    "    for i in range(len(new)):\n",
    "        new[i]['recipe_id'] = i + 1\n",
    "\n",
    "    with open(path+'/'+group_name+'.json', mode='w', encoding='utf-8') as f:\n",
    "        json.dump(new,f)\n",
    "\n",
    "    with open(path+'/'+group_name+'.json', mode = 'r', encoding = 'utf-8') as f: \n",
    "        output = json.load(f)\n",
    "    print('the len of new output is '+ str(len(output))) \n",
    "\n",
    "    output_old = []\n",
    "    allrecipes_list =  [path+'/'+group_name+'.json'] # other json files with same format can be inserted here.\n",
    "    print(allrecipes_list)\n",
    "    for i in range(len(allrecipes_list)):\n",
    "        with open(allrecipes_list[i], mode = 'r', encoding = 'utf-8') as f:\n",
    "            output_old.append(json.load(f))\n",
    "\n",
    "    '''this function is to merge separate json files into one json file and another csv file'''\n",
    "    def csvjson(allrecipes):\n",
    "\n",
    "        output  = []\n",
    "        table = pd.DataFrame(index = [\n",
    "                                'model','recipe_id','recipe_name','servings','group_name','calories',\n",
    "                                'protein','carbohydrates','fat','ingredients','recipe_source'\n",
    "                            ])\n",
    "\n",
    "        '''for allrecipes.com'''\n",
    "        for i in range(len(allrecipes)):\n",
    "            for recipeid in range(len(allrecipes[i])):\n",
    "                data = {'model': 'recipes.recipe',\n",
    "                        'recipe_id':recipeid+1,\n",
    "                        'recipe_name': allrecipes[i][recipeid]['recipe_name'],\n",
    "                        'servings': int(allrecipes[i][recipeid]['servings']),\n",
    "                        'group_name': allrecipes[i][recipeid]['group_name'],\n",
    "                        'calories': float(allrecipes[i][recipeid]['calories']),\n",
    "                        'protein': float(allrecipes[i][recipeid]['protein']),\n",
    "                        'carbohydrates': float(allrecipes[i][recipeid]['carbohydrates']),\n",
    "                        'fat': float(allrecipes[i][recipeid]['fat']),\n",
    "                        'ingredients': allrecipes[i][recipeid]['ingredients'],\n",
    "                        'recipe_source': allrecipes[i][recipeid]['recipe_source'],\n",
    "                       }\n",
    "                output.append(data)     \n",
    "\n",
    "                table[allrecipes[i][recipeid]['group_name']+'-id'+str(recipeid+1)] = [\n",
    "                    'recipes.recipe',recipeid+1,allrecipes[i][recipeid]['recipe_name'],int(allrecipes[i][recipeid]['servings']),\n",
    "                    allrecipes[i][recipeid]['group_name'],\n",
    "                    float(allrecipes[i][recipeid]['calories']),\n",
    "                    float(allrecipes[i][recipeid]['protein']),\n",
    "                    float(allrecipes[i][recipeid]['carbohydrates']),\n",
    "                    float(allrecipes[i][recipeid]['fat']),allrecipes[i][recipeid]['ingredients'],\n",
    "                    allrecipes[i][recipeid]['recipe_source']         \n",
    "                ]\n",
    "\n",
    "        table = table.stack()\n",
    "        table = table.unstack(0)\n",
    "        table.to_csv(path+'/'+group_name+'.csv')\n",
    "\n",
    "        print('Save as JSON file')\n",
    "        with open(path+'/'+group_name+'.json', mode='w', encoding='utf-8') as f:\n",
    "            json.dump(output, f)\n",
    "\n",
    "    csvjson(output_old) \n",
    "    with open(path+'/'+group_name+'.json', mode='r', encoding='utf-8') as f:\n",
    "        output = json.load(f)\n",
    "    print(len(output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
